{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6565db07",
   "metadata": {},
   "source": [
    "# DATA 304 â€” Session 2: Text Normalization\n",
    "\n",
    "This notebook mirrors the **Text Normalization** session and provides runnable examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39543d8",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bc06dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libs\n",
    "import re, unicodedata, math, random, string\n",
    "from collections import Counter\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP \n",
    "# !pip -q install contractions spacy pyspellchecker\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f28d7",
   "metadata": {},
   "source": [
    "## 1. Tiny sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc25c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = [\n",
    "    \"They're RUNNINGâ€”rÃ©sumÃ©!  \",\n",
    "    \"Data, data; DATA.\",\n",
    "    \"I can't, you won't.  \",\n",
    "    \"The leaves were falling and mice ran\",\n",
    "    \"ML Engineer / Data Scientist II\",\n",
    "    \"I love data ðŸ§ªðŸ“Š!  \",\n",
    "    \"colur is my favrite\",\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"id\": range(1, len(raw_texts)+1),\n",
    "    \"text\": raw_texts,\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce0936",
   "metadata": {},
   "source": [
    "## 2. Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b273cf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lower'] = df['text'].str.lower()\n",
    "df[['id','text','lower']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6c0d07",
   "metadata": {},
   "source": [
    "## 3. Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebabe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_punct'] = df['lower'].str.replace(r\"[^\\w\\s]\", \" \", regex=True)\n",
    "df[['id','lower','no_punct']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d7a9f2",
   "metadata": {},
   "source": [
    "## 4. Normalize whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df551d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ws_norm'] = df['no_punct'].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "df[['id','no_punct','ws_norm']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af4b255",
   "metadata": {},
   "source": [
    "## 5. Expand contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b080c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import contractions\n",
    "except Exception as e:\n",
    "    print(\"Install the 'contractions' package to run this cell: pip install contractions\")\n",
    "    raise\n",
    "\n",
    "df['expanded'] = df['ws_norm'].apply(contractions.fix)\n",
    "df[['id','ws_norm','expanded']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a95830",
   "metadata": {},
   "source": [
    "## 6. Strip accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8b0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def strip_accents(s: str) -> str:\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFKD\", s)\n",
    "        if not unicodedata.combining(c)\n",
    "    )\n",
    "\n",
    "df['no_accents'] = df['expanded'].apply(strip_accents)\n",
    "df[['id','expanded','no_accents']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1235e395",
   "metadata": {},
   "source": [
    "## 7. Remove emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8571e12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "emoji_pat = re.compile(r\"[\\U00010000-\\U0010FFFF]\")\n",
    "df['no_emoji'] = df['no_accents'].str.replace(emoji_pat, \"\", regex=True)\n",
    "df[['id','no_accents','no_emoji']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5aacb7",
   "metadata": {},
   "source": [
    "## 8. Lemmatization (spaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7280649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
    "    import subprocess, sys\n",
    "    _ = subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=False)\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatize(s: str) -> str:\n",
    "    doc = nlp(s)\n",
    "    return \" \".join(tok.lemma_ for tok in doc)\n",
    "\n",
    "df['lemma'] = df['no_emoji'].apply(lemmatize)\n",
    "df[['id','no_emoji','lemma']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b45c46",
   "metadata": {},
   "source": [
    "## 9. Stemming (NLTK PorterStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beea804",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from nltk.stem import PorterStemmer\n",
    "except Exception as e:\n",
    "    print(\"Install NLTK to run this cell: pip install nltk\")\n",
    "    raise\n",
    "\n",
    "ps = PorterStemmer()\n",
    "df['stem'] = df['no_emoji'].str.lower().str.split().apply(lambda toks: \" \".join(ps.stem(w) for w in toks))\n",
    "df[['id','no_emoji','stem']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e35d80",
   "metadata": {},
   "source": [
    "## 10. Standardize spelling (pyspellchecker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de4790",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from spellchecker import SpellChecker\n",
    "except Exception as e:\n",
    "    print(\"Install pyspellchecker to run this cell: pip install pyspellchecker\")\n",
    "    raise\n",
    "\n",
    "sp = SpellChecker(distance=1)\n",
    "def correct_spelling(s: str) -> str:\n",
    "    toks = s.split()\n",
    "    corr = [sp.correction(w) or w for w in toks]\n",
    "    return \" \".join(corr)\n",
    "\n",
    "df['spcorr'] = df['lemma'].apply(correct_spelling)\n",
    "df[['id','lemma','spcorr']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d78b9a",
   "metadata": {},
   "source": [
    "## 11. Pipeline: putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacacb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import contractions\n",
    "def normalize_pipeline(text: str, nlp_model) -> str:\n",
    "    t = text.lower()\n",
    "    t = contractions.fix(t)\n",
    "    t = re.sub(r\"[^\\w\\s]\", \" \", t)\n",
    "    t = strip_accents(t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    doc = nlp_model(t)\n",
    "    t = \" \".join(tok.lemma_ for tok in doc)\n",
    "    return t\n",
    "\n",
    "df['norm'] = df['text'].apply(lambda s: normalize_pipeline(s, nlp))\n",
    "df[['id','text','norm']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f98ab21",
   "metadata": {},
   "source": [
    "## 12. Measuring impact of normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdce1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def tokenize(s: str):\n",
    "    return re.findall(r\"\\w+\", s.lower())\n",
    "\n",
    "def charset_size(texts):\n",
    "    return len(set(\"\".join(texts)))\n",
    "\n",
    "raw = df['text'].tolist()\n",
    "clean = df['norm'].tolist()\n",
    "\n",
    "# Unique docs\n",
    "unique_before = len(set(raw))\n",
    "unique_after  = len(set(clean))\n",
    "\n",
    "# Frequencies\n",
    "freq_before = Counter(tok for t in raw for tok in tokenize(t))\n",
    "freq_after  = Counter(tok for t in clean for tok in tokenize(t))\n",
    "\n",
    "# Charset\n",
    "chars_before = charset_size(raw)\n",
    "chars_after  = charset_size(clean)\n",
    "\n",
    "# Effects\n",
    "vocab_before = set(freq_before)\n",
    "vocab_after  = set(freq_after)\n",
    "vocab_shrink = (1 - len(vocab_after) / max(1, len(vocab_before)))\n",
    "jaccard_vocab = len(vocab_before & vocab_after) / max(1, len(vocab_before | vocab_after))\n",
    "\n",
    "summary = pd.Series({\n",
    "    \"unique_docs_before\": unique_before,\n",
    "    \"unique_docs_after\": unique_after,\n",
    "    \"tokens_before\": sum(freq_before.values()),\n",
    "    \"tokens_after\": sum(freq_after.values()),\n",
    "    \"vocab_size_before\": len(vocab_before),\n",
    "    \"vocab_size_after\": len(vocab_after),\n",
    "    \"vocab_shrink_fraction\": round(vocab_shrink, 3),\n",
    "    \"jaccard_vocab_overlap\": round(jaccard_vocab, 3),\n",
    "    \"charset_size_before\": chars_before,\n",
    "    \"charset_size_after\": chars_after,\n",
    "    \"charset_reduction\": chars_before - chars_after,\n",
    "}).to_frame(\"value\")\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a376d9eb",
   "metadata": {},
   "source": [
    "## 13. Visualize vocabulary change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec039451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = [\"Before\", \"After\"]\n",
    "sizes = [summary.loc['vocab_size_before','value'], summary.loc['vocab_size_after','value']]\n",
    "plt.figure()\n",
    "plt.bar(labels, sizes)\n",
    "plt.title(\"Vocabulary Size: Before vs After\")\n",
    "plt.xlabel(\"Stage\")\n",
    "plt.ylabel(\"Unique tokens\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c454985",
   "metadata": {},
   "source": [
    "## 14. Semantic normalization: dictionary-based mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817c7b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = pd.Series([\n",
    "    \"Data Scientist II\",\n",
    "    \"Senior Data Analyst\",\n",
    "    \"ML Engineer\",\n",
    "    \"machine learning engineer\",\n",
    "    \"data analyst iii\",\n",
    "    \"Director of Data Science\"\n",
    "], name=\"role_clean\")\n",
    "\n",
    "map_roles = {\n",
    "    \"data scientist\": \"ds\",\n",
    "    \"data analyst\": \"da\",\n",
    "    \"machine learning engineer\": \"ml\"\n",
    "}\n",
    "\n",
    "def clean_role_simple(s: str) -> str:\n",
    "    s2 = re.sub(r\"[^a-zA-Z\\s/+-]\", \" \", s.lower())\n",
    "    s2 = re.sub(r\"\\s+\", \" \", s2).strip()\n",
    "    return s2\n",
    "\n",
    "role_norm = roles.apply(clean_role_simple)\n",
    "\n",
    "# Exact mapping, then fallback\n",
    "mapped = role_norm.replace(map_roles).where(lambda x: x.isin(map_roles.values()), other=\"other\")\n",
    "pd.DataFrame({\"raw\": roles, \"cleaned\": role_norm, \"role_std\": mapped})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046f2361",
   "metadata": {},
   "source": [
    "## 15. Regex-based semantic mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eba247",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = {\n",
    "    r'^data\\s+scientist(\\b.*)?$': 'ds',\n",
    "    r'^data\\s+analyst(\\b.*)?$': 'da',\n",
    "    r'^(ml|m/?l|machine\\s*learning)\\s+engineer(\\b.*)?$': 'ml',\n",
    "}\n",
    "role_std_regex = role_norm.replace(patterns, regex=True).fillna(\"other\")\n",
    "\n",
    "\n",
    "display(pd.DataFrame({\n",
    "    \"raw\": roles,\n",
    "    \"cleaned\": role_norm,\n",
    "    \"role_std\": role_std_regex\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b4107f",
   "metadata": {},
   "source": [
    "## 16. Multi-field cleaning workflow demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade72c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = pd.DataFrame({\n",
    "    \"address\": [\"123 Main St, Knoxville, TN 37996\", \"456 2nd Ave, Nashville, TN 37209\"],\n",
    "    \"city_ref\": [\"knoxville\", \"nashville\"]\n",
    "})\n",
    "\n",
    "profile = demo.describe(include='object')\n",
    "display(profile)\n",
    "\n",
    "demo['city'] = demo['address'].str.extract(r'(,[A-Za-z\\s]+),').iloc[:,0].str.strip()\n",
    "demo['zip']  = demo['address'].str.extract(r'(\\d{5})').iloc[:,0]\n",
    "demo['city_norm'] = demo['city'].str.lower().str.replace(r\"\\s+\", \" \", regex=True).str.replace(\",\", \"\", regex=False).str.strip()\n",
    "\n",
    "ref = pd.DataFrame({\"city_norm\": [\"knoxville\", \"nashville\"], \"valid_city\": [True, True]})\n",
    "demo = demo.merge(ref, on=\"city_norm\", how=\"left\", indicator=True)\n",
    "demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcf6de5",
   "metadata": {},
   "source": [
    "## 17. Performance tips and quick profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dc92cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "big = pd.DataFrame({\"txt\": np.random.choice(df['text'], size=20000, replace=True)})\n",
    "\n",
    "def vec_clean(s: pd.Series) -> pd.Series:\n",
    "    return (s.str.lower()\n",
    "              .str.replace(r\"[^\\w\\s]\", \" \", regex=True)\n",
    "              .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "              .str.strip())\n",
    "\n",
    "def py_apply_clean(s: pd.Series) -> pd.Series:\n",
    "    def f(x):\n",
    "        t = x.lower()\n",
    "        t = re.sub(r\"[^\\w\\s]\", \" \", t)\n",
    "        t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "        return t\n",
    "    return s.apply(f)\n",
    "\n",
    "print(\"Vectorized:\")\n",
    "get_ipython().run_line_magic('timeit', 'vec_clean(big[\\'txt\\'])')\n",
    "print(\"Python apply:\")\n",
    "get_ipython().run_line_magic('timeit', 'py_apply_clean(big[\\'txt\\'])')\n",
    "\n",
    "big.memory_usage(deep=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
